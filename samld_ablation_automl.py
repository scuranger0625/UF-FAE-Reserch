import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import unix_timestamp, concat_ws, col
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml.classification import (
    LogisticRegression, DecisionTreeClassifier,
    RandomForestClassifier, LinearSVC
)
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline

# === 1. å»ºç«‹ SparkSessionï¼ˆå”¯ä¸€å…¥å£ï¼‰===
spark = SparkSession.builder.appName("SAML-D All-Mode Ablation ML with TimeSeries Split").getOrCreate()
spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")  # é˜²æ­¢èˆŠç‰ˆ datetime è§£æéŒ¯èª¤

# === 2. è®€å– parquet æª”æ¡ˆï¼ˆå¤šæ¨¡æ…‹/åŸå§‹ï¼‰===
df_full = spark.read.parquet("gs://saml-d/SAML-D_with_graph_centrality.parquet")  # å¤šæ¨¡æ…‹/åœ–è«–ç‰¹å¾µ
df_orig = spark.read.parquet("gs://saml-d/SAML-D.parquet")                       # ç´”åŸå§‹ç‰¹å¾µ

# === 3. çµ±ä¸€åŠ  timestamp æ¬„ä½ï¼ˆåˆ©æ–¼æ™‚é–“æ’åºåˆ‡åˆ†ï¼‰===
def add_timestamp(df):
    return df.withColumn(
        "timestamp",
        unix_timestamp(concat_ws(" ", col("Date"), col("Time")), "yyyy-MM-dd HH:mm:ss").cast("long")
    )

df_full = add_timestamp(df_full)
df_orig = add_timestamp(df_orig)

# === 4. æŒ‰ timestamp åš´æ ¼æ’åºï¼Œåˆ‡ 80/20 è¨“ç·´æ¸¬è©¦ ===
def split_by_time(df):
    df = df.orderBy("timestamp")
    total = df.count()
    split_idx = int(total * 0.8)
    train_data = df.limit(split_idx)
    test_data = df.subtract(train_data)
    return train_data, test_data

train_full, test_full = split_by_time(df_full)
train_orig, test_orig = split_by_time(df_orig)

# === 5. ä¸‰ç¨®ç‰¹å¾µçµ„åˆï¼ˆæ¶ˆèå¯¦é©—åˆ†çµ„ï¼‰===
modes = {
    "å¤šæ¨¡æ…‹(åŸå§‹+åœ–è«–)": {
        "df_train": train_full,
        "df_test": test_full,
        "categorical_cols": [
            "Payment_currency", "Received_currency",
            "Sender_bank_location", "Receiver_bank_location", "Payment_type"
        ],
        "numeric_cols": [
            "Amount",
            "group_node_count", "group_edge_count", "group_bidirect_ratio",
            "sender_degree", "receiver_degree",
            "sender_closeness", "receiver_closeness",
            "sender_betweenness", "receiver_betweenness"
        ]
    },
    "ç´”åŸå§‹æ¬„ä½": {
        "df_train": train_orig,
        "df_test": test_orig,
        "categorical_cols": ["Payment_type"],
        "numeric_cols": ["Amount"]
    },
    "ç´”åœ–è«–ç‰¹å¾µ": {
        "df_train": train_full,
        "df_test": test_full,
        "categorical_cols": [],
        "numeric_cols": [
            "group_node_count", "group_edge_count", "group_bidirect_ratio",
            "sender_degree", "receiver_degree",
            "sender_closeness", "receiver_closeness",
            "sender_betweenness", "receiver_betweenness"
        ]
    }
}

# === 6. å®šç¾© ML ç¶“å…¸æ¨¡å‹ï¼ˆå››ç¨®ï¼‰===
models = {
    "Logistic Regression": LogisticRegression(labelCol="Is_laundering", featuresCol="features"),
    "Decision Tree": DecisionTreeClassifier(labelCol="Is_laundering", featuresCol="features"),
    "Random Forest": RandomForestClassifier(labelCol="Is_laundering", featuresCol="features", numTrees=100),
    "SVM (LinearSVC)": LinearSVC(labelCol="Is_laundering", featuresCol="features")
}

# === 7. çµ±ä¸€æŒ‡æ¨™å‡½æ•¸ï¼ˆWeighted, AUC, Support çš†é½Šå…¨ï¼‰===
def evaluate_metrics(predictions):
    # ====== ROC AUC & weighted æŒ‡æ¨™ ======
    auc = BinaryClassificationEvaluator(labelCol="Is_laundering", metricName="areaUnderROC").evaluate(predictions)
    precision_weighted = MulticlassClassificationEvaluator(
        labelCol="Is_laundering", predictionCol="prediction", metricName="weightedPrecision"
    ).evaluate(predictions)
    recall_weighted = MulticlassClassificationEvaluator(
        labelCol="Is_laundering", predictionCol="prediction", metricName="weightedRecall"
    ).evaluate(predictions)
    f1_weighted = MulticlassClassificationEvaluator(
        labelCol="Is_laundering", predictionCol="prediction", metricName="f1"
    ).evaluate(predictions)
    return auc, precision_weighted, recall_weighted, f1_weighted

def eval_class_metrics(predictions, cls):
    # ====== å„é¡åˆ¥ Precision/Recall/F1 ======
    prec = MulticlassClassificationEvaluator(
        labelCol="Is_laundering", predictionCol="prediction", metricName="precisionByLabel"
    ).setMetricLabel(cls).evaluate(predictions)
    rec = MulticlassClassificationEvaluator(
        labelCol="Is_laundering", predictionCol="prediction", metricName="recallByLabel"
    ).setMetricLabel(cls).evaluate(predictions)
    f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0
    return prec, rec, f1

def get_support(predictions):
    # ====== Support: å„é¡åˆ¥æ¨£æœ¬æ•¸ ======
    return predictions.groupBy("Is_laundering").count().toPandas().set_index("Is_laundering")["count"].to_dict()

# === 8. ä¸»ç¨‹å¼åŸ·è¡Œ ===
for mode_name, cfg in modes.items():
    print("=" * 35)
    print(f"ğŸŸ© æ¶ˆèåˆ†çµ„ï¼š{mode_name}")
    categorical_cols = cfg["categorical_cols"]
    numeric_cols = cfg["numeric_cols"]
    train_data = cfg["df_train"]
    test_data = cfg["df_test"]

    # === ç‰¹å¾µå·¥ç¨‹ï¼ˆé¡åˆ¥ç·¨ç¢¼/æ•¸å€¼æ‹¼æ¥ï¼‰===
    indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_idx", handleInvalid="keep") for c in categorical_cols]
    encoders = [OneHotEncoder(inputCol=f"{c}_idx", outputCol=f"{c}_vec") for c in categorical_cols]
    feature_cols = numeric_cols + [f"{c}_vec" for c in categorical_cols]
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

    for model_name, clf in models.items():
        print(f"\nğŸ”¹ã€{mode_name} | {model_name}ã€‘")
        start = time.time()
        # ç„¡é¡åˆ¥ç‰¹å¾µå°±ä¸åŠ ç·¨ç¢¼å™¨
        stages = indexers + encoders + [assembler, clf] if categorical_cols else [assembler, clf]
        pipeline = Pipeline(stages=stages)
        model = pipeline.fit(train_data)
        predictions = model.transform(test_data)
        elapsed = time.time() - start

        # ==== ä¸»è¦æŒ‡æ¨™ ====
        auc, p_weighted, r_weighted, f1_weighted = evaluate_metrics(predictions)
        print(f"   ğŸ•’ è¨“ç·´+é æ¸¬æ™‚é–“   ï¼š{elapsed:.2f} ç§’")
        print(f"   ğŸ“ˆ AUC(ROC)        ï¼š{auc:.4f}")
        print(f"   ğŸ¯ Weighted Precisionï¼š{p_weighted:.4f}")
        print(f"   ğŸ¯ Weighted Recall   ï¼š{r_weighted:.4f}")
        print(f"   ğŸ§® Weighted F1 Score ï¼š{f1_weighted:.4f}")

        # ==== support æŒ‡æ¨™ ====
        support_dict = get_support(predictions)
        print(f"   ğŸ”¢ Class Support     ï¼š{support_dict}")

        # ==== å„é¡åˆ¥æŒ‡æ¨™ ====
        for cls in [0.0, 1.0]:
            prec, rec, f1 = eval_class_metrics(predictions, cls)
            sup = support_dict.get(cls, 0)
            print(f"   ğŸ”¹ Class {int(cls)} â€” Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, Support: {sup}")

        # ==== ç‰¹å¾µé‡è¦æ€§ or ä¿‚æ•¸ ====
        if model_name in ["Decision Tree", "Random Forest"]:
            importances = model.stages[-1].featureImportances
            feature_names = numeric_cols.copy()
            for c in categorical_cols:
                ohe_size = 20  # OHE å‘é‡ç¶­åº¦ï¼Œå¯¦å‹™å¯è‡ªå‹•æŠ“é•·åº¦
                feature_names.extend([f"{c}_vec_{i}" for i in range(ohe_size)])
            sorted_features = sorted(zip(feature_names, list(importances)), key=lambda x: -x[1])[:10]
            print("   ğŸ”¬ Feature Importances:")
            for fname, score in sorted_features:
                print(f"      {fname:<25} {score:.4f}")
        elif model_name == "Logistic Regression":
            coefs = model.stages[-1].coefficients.toArray()
            top_idx = abs(coefs).argsort()[-10:][::-1]
            print("   ğŸ”¬ Top 10 Coefficients:")
            for idx in top_idx:
                if idx < len(numeric_cols):
                    feat_name = numeric_cols[idx]
                else:
                    feat_name = f"encoded_{idx}"
                print(f"      {feat_name:<25} abs(coef): {abs(coefs[idx]):.4f}")
        elif model_name == "SVM (LinearSVC)":
            coefs = model.stages[-1].coefficients.toArray()
            top_idx = abs(coefs).argsort()[-10:][::-1]
            print("   ğŸ”¬ Top 10 Coefficients:")
            for idx in top_idx:
                if idx < len(numeric_cols):
                    feat_name = numeric_cols[idx]
                else:
                    feat_name = f"encoded_{idx}"
                print(f"      {feat_name:<25} abs(coef): {abs(coefs[idx]):.4f}")
        print()

print("ğŸ‰ã€ä¸‰å¤§æ¶ˆèåˆ†çµ„ï¼Œå®Œæ•´ä¸»æŒ‡æ¨™+ç‰¹å¾µé‡è¦æ€§+supportæŒ‡æ¨™ï¼Œå…¨è‡ªå‹•æ¯”è¼ƒå®Œç•¢ï¼ã€‘")
