# -*- coding: utf-8 -*-
"""
UF-FAEï¼ˆç„¡ MLï¼‰â€” Polars ç‰ˆ + äº’æƒ ç²¾ç…‰ + æŒ‡æ¨™é¢æ¿ + è¿‘ä¼¼æº–ç¢ºæ€§è¨ºæ–·

æœ¬ç¨‹å¼ç‰¹è‰²ï¼š
1) æµå¼å¼±é€£é€š (WCC)ï¼šUnion-Findï¼ˆDSUï¼‰åœ¨é‚Šåˆ°é”æ™‚å³æ™‚åˆä½µã€‚
2) è¿‘ä¼¼ç‰¹å¾µï¼š
   - KMVï¼ˆK-Minimum Valuesï¼‰ï¼šä¼°è¨ˆç¾¤çµ„çš„å»é‡ç¯€é»æ•¸ï¼ˆdistinctï¼‰ã€‚
   - Count-Min Sketchï¼ˆCMSï¼‰ï¼šä¼°è¨ˆç¯€é»çš„å‡º/å…¥é‡‘é¡ï¼ˆheavy hittersï¼‰ã€‚
3) äº’æƒ ç²¾ç…‰ï¼šç•¶ç¾¤çµ„åˆ†æ•¸ S é«˜æ–¼é–€æª»ï¼Œæ–¼æœ€è¿‘çª—å£å…§ç”¨ã€Œäº’æƒ é‚Š (uâ†’v & vâ†’u)ã€æŠŠ WCC æ”¶ç·Šæˆæœ‰å‘å­å¡Šã€‚
4) Robust z-like åˆ†æ•¸ï¼šç”¨ä¸­ä½æ•¸+MAD å° distinct / out_hh / density åšç©©å¥æ¨™æº–åŒ–ï¼ŒS=0.4*zD+0.4*zHH+0.2*zÏã€‚
5) æŒ‡æ¨™é¢æ¿ï¼š
   - å®šæœŸå°å‡ºå…ƒä»¶æ•¸ã€æœ€å¤§ç¾¤çµ„è¦æ¨¡ã€æœ€ç·Šç¾¤çµ„å¯†åº¦ã€Top-K å‡ºé‡‘é‡æ“Šè€…ï¼ˆæŠ½æ¨£ï¼‰ã€æœ€è¿‘ä¸€æ¬¡åˆä½µçš„ S æ˜ç´°ã€‚
   - ä»¥ç´…ğŸŸ¥/æ©˜ğŸŸ§/ç¶ ğŸŸ©ç‡ˆé¡¯ç¤ºé¢¨éšªç­‰ç´šèˆ‡ä¸€è¡Œè¨ºæ–·èªªæ˜ã€‚
6) è¿‘ä¼¼ç®—æ³•æº–ç¢ºæ€§æŒ‡æ¨™ï¼ˆç†è«– + å³æ™‚ï¼‰ï¼š
   - KMVï¼škã€æ¨£æœ¬æ•¸ã€æœ«ç§© R_kã€é æœŸç›¸å°èª¤å·® ~ 1/sqrt(k)ï¼ˆå‚™è¨»ï¼šç†è«–å¸¸æ•¸ç•¥å› å¯¦ä½œè€Œç•°ï¼‰ã€‚
   - CMSï¼šwã€dã€Îµ=1/wã€Î´=e^{-d}ã€ç•¶å‰ç¸½æµé‡ Nï¼ˆå‡ºã€å…¥ï¼‰ã€å°ä»»æ„æŸ¥è©¢ä¼°è¨ˆçš„åå·®ä¸Šç•Œ Îµ*Nã€‚

åŸ·è¡Œå‰å®‰è£ï¼š
  pip install -U polars tqdm
ï¼ˆWindows å»ºè­°ï¼šè¨­å®šç’°å¢ƒè®Šæ•¸ POLARS_MAX_THREADS=16ã€RAYON_NUM_THREADS=16 ä»¥åŠ é€Ÿ Parquet è§£å£“ï¼Œä½† Python ä¸»è¿´åœˆä»æ˜¯å–®åŸ·è¡Œç·’ï¼‰

âš ï¸ æ³¨æ„ï¼š
- CMS å› å“ˆå¸Œç¢°æ’åªã€Œé«˜ä¼°ä¸ä½ä¼°ã€ï¼Œæˆ‘å€‘åˆ—å°çš„æ˜¯ã€Œå¯èƒ½çš„ä¸Šç•Œåå·®ã€ä¾›ä½ åˆ¤è®€ã€‚
- KMV çš„èª¤å·®å±¬æ–¼æœŸæœ›èˆ‡æ¼¸è¿‘ç‰¹æ€§ï¼›æˆ‘å€‘çµ¦ä½ ã€Œç†è«–ç´šã€å¿«é€ŸæŒ‡æ¨™ï¼Œä¸æ˜¯çµ•å°ä¿è­‰ã€‚
"""

# =========================
# æ‰€æœ‰ import éƒ½åœ¨æœ€å‰é¢
# =========================
import os
import csv
import math
import bisect
from typing import Any, Deque, Dict, Tuple, List, Set, Optional
from collections import deque, defaultdict

import polars as pl
from tqdm.auto import tqdm


# =========================
# ä½¿ç”¨è€…å¯èª¿åƒæ•¸
# =========================

# ---- æª”æ¡ˆèˆ‡æ¬„ä½ ----
INPUT_PARQUET = r"C:\Users\Leon\Desktop\ç¨‹å¼èªè¨€è³‡æ–™\python\UF-FAE\Anti Money Laundering Transaction Data (SAML-D)\SAML-D.parquet"
OUTPUT_ALERTS = "alerts_out.csv"
ASSUME_SORTED = True   # è‹¥ parquet å·²æŒ‰æ™‚é–“æ’åºï¼ŒTrue å¯çœæ’åºèˆ‡è¨˜æ†¶é«”
TIME_COL = "time"      # ç›®æ¨™æ™‚é–“æ¬„ä½ï¼ˆæœƒè‡ªå‹•å˜—è©¦åˆ¥åï¼‰
SENDER_COL = "sender"
RECEIVER_COL = "receiver"
AMOUNT_COL = "amount"

# ---- è¿‘ä¼¼èˆ‡è¦–çª—è¨­å®š ----
KMV_K = 128           # KMV æ¨£æœ¬æ•¸ï¼ˆè¶Šå¤§è¶Šæº–ï¼Œè¨˜æ†¶é«” O(K)ï¼‰
CMS_W = 4096          # CMS å¯¬åº¦ï¼ˆèª¤å·® Îµ â‰ˆ 1/Wï¼‰
CMS_D = 6             # CMS æ·±åº¦ï¼ˆéŒ¯èª¤æ©Ÿç‡ Î´ â‰ˆ e^{-D}ï¼‰
WINDOW_EDGES = 2_000_000  # åªä¿ç•™æœ€è¿‘é€™éº¼å¤šã€Œå·²å¥—ç”¨ã€çš„é‚Šï¼ˆä¾›ç²¾ç…‰è§€å¯Ÿï¼‰
GAP_SIZE = 10_000         # å»¶é²è™•ç†ï¼Œå¸æ”¶äº‚åºï¼ˆpending queueï¼‰

# ---- åˆ¤è­¦é–€æª»ï¼ˆrobust z-likeï¼‰----
THRESH_MID = 2.0
THRESH_HIGH = 3.0

# ---- äº’æƒ ç²¾ç…‰è¨­å®š ----
REFINE_ON_S = True
REFINE_S_THRESH = THRESH_HIGH   # é«˜æ–¼æ­¤ S é–€æª»æ‰è§¸ç™¼ç²¾ç…‰
RECIP_WINDOW = 200_000          # æ”¶é›†æœ€è¿‘é€™éº¼å¤šæ¢å±¬æ–¼è©²å…ƒä»¶çš„é‚Šä¾†åˆ¤äº’æƒ 
MIN_SUBCOMP_NODES = 2           # å¤ªå°çš„å­å¡Šä¸Ÿæ‰

# ---- æŒ‡æ¨™é¢æ¿èˆ‡æ•ˆèƒ½å¾®èª¿ ----
PRINT_METRICS_EVERY = 200_000   # æ¯è™•ç†å¤šå°‘é‚Šåˆ—å°ä¸€æ¬¡é¢æ¿
RECENT_SENDER_BUFFER = 10_000   # æŠ½æ¨£é‡æ“Šè€…ï¼šåªæƒæœ€è¿‘é€™äº› sender
TOPK_HEAVY_HITTERS = 5          # é¡¯ç¤ºå‰ K åå‡ºé‡‘é‡æ“Šè€…
TQDM_UPDATE_EVERY = 1_000       # é€²åº¦æ¢æ¯ 1k ç­†æ‰æ›´æ–°ä¸€æ¬¡ï¼Œé¿å…å¤ªæ…¢
CSV_FLUSH_EVERY   = 50_000      # è­¦ç¤ºç´¯ç©åˆ° 5 è¬ç­†æ‰æ‰¹æ¬¡å¯«æª”


# =========================
# è³‡æ–™çµæ§‹ï¼šDSU / KMV / CMS
# =========================

class DSU:
    """ç¶“å…¸ Union-Findï¼›ç¶­è­·å¼±é€£é€šï¼ˆæŠŠåœ–ç•¶ç„¡å‘ï¼‰ï¼Œè¿‘ä¼¼ O(Î±(n))ã€‚"""
    def __init__(self):
        self.parent: Dict[Any, Any] = {}
        self.rank: Dict[Any, int] = {}

    def find(self, x):
        if x not in self.parent:
            self.parent[x] = x
            self.rank[x] = 0
            return x
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])  # è·¯å¾‘å£“ç¸®
        return self.parent[x]

    def union(self, a, b):
        ra, rb = self.find(a), self.find(b)
        if ra == rb:
            return False, ra, rb
        # æŒ‰ç§©åˆä½µ
        if self.rank[ra] < self.rank[rb]:
            ra, rb = rb, ra
        self.parent[rb] = ra
        if self.rank[ra] == self.rank[rb]:
            self.rank[ra] += 1
        return True, ra, rb

    def reset_subset(self, nodes: Set[Any]):
        """äº’æƒ ç²¾ç…‰ç”¨ï¼šæŠŠä¸€çµ„ç¯€é»è¦–ç‚ºå„è‡ªç¨ç«‹ï¼Œä¹‹å¾Œå†ä¾äº’æƒ  unionã€‚"""
        for x in nodes:
            self.parent[x] = x
            self.rank[x] = 0


class KMV:
    """
    KMVï¼ˆK-Minimum Valuesï¼‰å»é‡ä¼°è¨ˆï¼š
    - åªä¿ç•™ hash å¾Œæœ€å°çš„ K å€‹å€¼ï¼ˆéå¢æ’åºï¼‰
    - ä¼°è¨ˆ distinct â‰ˆ (K-1) * (U / R_k)ï¼ŒU=2^64ï¼ŒR_k=ç¬¬Kå°å€¼
    - é æœŸç›¸å°èª¤å·® ~ O(1/sqrt(K))ï¼ˆç†è«–è¿‘ä¼¼ï¼‰
    """
    def __init__(self, k: int = 64):
        self.k = k
        self.samples: List[int] = []  # å·²æ’åºï¼ˆbisect æ’å…¥ï¼‰

    @staticmethod
    def _hv(x) -> int:
        return hash(x) & 0xffffffffffffffff  # 64-bit éè² 

    def add(self, x):
        hv = KMV._hv(x)
        bisect.insort(self.samples, hv)
        if len(self.samples) > self.k:
            self.samples.pop()

    def estimate(self) -> float:
        n = len(self.samples)
        if n < self.k:
            # æ¨£æœ¬ä¸è¶³æ™‚ï¼Œç›´æ¥ä»¥æ¨£æœ¬æ•¸ä½œç‚ºä¸‹ç•Œ
            return float(n)
        U = float(1 << 64)
        r_k = float(self.samples[-1])  # ç¬¬ K å°
        if r_k <= 0.0:
            return float(n)
        return (self.k - 1) * (U / r_k)

    # ---- è¨ºæ–·ï¼šKMV æº–ç¢ºæ€§è¿‘ä¼¼ ----
    def diagnostics(self) -> Dict[str, Optional[float]]:
        """
        å›å‚³ KMV è¿‘ä¼¼æº–ç¢ºæ€§æŒ‡æ¨™ï¼ˆç†è«–ç´šå¿«é€Ÿä¼°è¨ˆï¼‰ï¼š
        - kï¼šæ¨£æœ¬ä¸Šé™
        - n_samplesï¼šç›®å‰æ¨£æœ¬æ•¸ï¼ˆè‹¥å°šæœªé” kï¼Œä»£è¡¨ distinct é‚„å°ï¼‰
        - r_kï¼šç›®å‰ç¬¬ k å°å€¼ï¼ˆæ¨£æœ¬ä¸è¶³æ™‚ç‚º Noneï¼‰
        - rel_error_theoryï¼šç†è«–ç´šç›¸å°èª¤å·® ~ c / sqrt(k)ï¼Œé€™è£¡ç”¨ 1/sqrt(k) åšä¿å®ˆä¼°
        """
        n = len(self.samples)
        r_k = float(self.samples[-1]) if n >= self.k else None
        rel_err = 1.0 / math.sqrt(self.k) if self.k > 0 else None
        return {
            "k": float(self.k),
            "n_samples": float(n),
            "r_k": float(r_k) if r_k is not None else None,
            "rel_error_theory": float(rel_err) if rel_err is not None else None,
        }


class CountMinSketch:
    """
    Count-Min Sketchï¼š
    - d å€‹ hash å‡½æ•¸å°æ‡‰ d è¡Œï¼Œw æ¬„å¯¬ï¼›æ›´æ–°èˆ‡æŸ¥è©¢éƒ½æ˜¯ O(d) â‰ˆ O(1)ã€‚
    - æŸ¥è©¢å– d å€‹ bucket çš„æœ€å°å€¼ï¼Œç‚ºçœŸå€¼ä¸Šç•Œï¼ˆåªé«˜ä¼°ä¸ä½ä¼°ï¼‰ã€‚
    - èª¤å·®ä¸Šç•Œï¼šf_hat(x) <= f(x) + Îµ * Nï¼ŒÎµ â‰ˆ 1/wï¼Œæ©Ÿç‡è‡³å°‘ 1-Î´ï¼ˆÎ´ â‰ˆ e^{-d}ï¼‰ã€‚
    """
    def __init__(self, width=2048, depth=4, seed=1315423911):
        self.w = width
        self.d = depth
        self.tables = [[0]*self.w for _ in range(self.d)]
        self.seeds = [(seed * (i+1)) & 0xffffffff for i in range(self.d)]

    def _h(self, i, x) -> int:
        # æ³¨æ„ï¼šPython hash ä¸¦é 2-universalï¼Œé€™è£¡åšå·¥ç¨‹è¿‘ä¼¼ï¼ˆå¦‚éœ€åš´è¬¹å¯ç”¨ mmh3ï¼‰
        h = (hash(x) ^ self.seeds[i]) & 0x7fffffff
        return h % self.w

    def update(self, key, val=1):
        v = int(val)
        for i in range(self.d):
            j = self._h(i, key)
            self.tables[i][j] += v

    def query(self, key) -> int:
        est = None
        for i in range(self.d):
            cur = self.tables[i][self._h(i, key)]
            est = cur if est is None else min(est, cur)
        return est or 0


# =========================
# ç¾¤çµ„çµ±è¨ˆï¼šComponentStats
# =========================

class ComponentStats:
    """å°æ¯å€‹ DSU ç¾¤çµ„ç´¯è¨ˆ KMV/CMS èˆ‡ç°¡æ˜“å¯†åº¦ã€æ­·å²ï¼Œä»¥ä¾›æ‰“åˆ†èˆ‡è¨ºæ–·ã€‚"""
    def __init__(self, kmv_k=64, cms_w=2048, cms_d=4):
        self.kmv = KMV(k=kmv_k)
        self.cms_out = CountMinSketch(width=cms_w, depth=cms_d)
        self.cms_in  = CountMinSketch(width=cms_w, depth=cms_d)
        self.nodes: Set[Any] = set()
        self.edge_cnt = 0
        self.total_out_flow = 0.0   # ä¾› CMS èª¤å·®ä¸Šç•Œä½¿ç”¨ï¼ˆN_outï¼‰
        self.total_in_flow  = 0.0   # ä¾› CMS èª¤å·®ä¸Šç•Œä½¿ç”¨ï¼ˆN_inï¼‰
        self.history: Deque[Tuple[float,float,float]] = deque(maxlen=256)  # (distinct, out_hh(src), density)

    def update_on_edge(self, u, v, amount):
        """æ¯æ¢é‚Šæ›´æ–° KMV / CMS / ç¯€é»é›†åˆ / é‚Šæ•¸ã€‚"""
        self.kmv.add(u); self.kmv.add(v)
        self.nodes.add(u); self.nodes.add(v)
        self.edge_cnt += 1
        w = max(1.0, float(amount))  # é˜² 0 æˆ–è² æ•¸ï¼Œç•¶ä½œäº¤æ˜“å¼·åº¦
        self.cms_out.update(u, val=w)
        self.cms_in.update(v,  val=w)
        self.total_out_flow += w
        self.total_in_flow  += w

    # --- è¿‘ä¼¼æŒ‡æ¨™ ---
    def approx_distinct(self) -> float:
        return self.kmv.estimate()

    def approx_density(self) -> float:
        n = len(self.nodes)
        if n < 2: return 0.0
        # ç„¡å‘åœ–å¯†åº¦è¿‘ä¼¼ï¼š2E / (N*(N-1))
        return (2.0 * float(self.edge_cnt)) / (float(n) * float(n-1))

    def approx_out_hh(self, key) -> float:
        return float(self.cms_out.query(key))

    # --- robust z-like ---
    @staticmethod
    def _robust_z(x, series: List[float]) -> float:
        if not series: return 0.0
        data = sorted(series)
        m = data[len(data)//2]
        dev = sorted(abs(xx - m) for xx in data)
        mad = dev[len(dev)//2] if dev else 1.0
        denom = max(1.4826 * mad, 1e-9)
        return (x - m) / denom

    def snapshot_and_score(self, src_key: Any) -> Tuple[float, Dict[str, float]]:
        """æ“·å–ç•¶ä¸‹ 3 æŒ‡æ¨™ + z åˆ†æ•¸ï¼Œå›å‚³ S èˆ‡æ˜ç´°ã€‚"""
        d = self.approx_distinct()
        hh = self.approx_out_hh(src_key)
        dens = self.approx_density()
        z1 = ComponentStats._robust_z(d,   [h[0] for h in self.history])
        z2 = ComponentStats._robust_z(hh,  [h[1] for h in self.history])
        z3 = ComponentStats._robust_z(dens,[h[2] for h in self.history])
        S = 0.4*z1 + 0.4*z2 + 0.2*z3
        self.history.append((d, hh, dens))
        return S, {
            "distinct": d, "out_hh": hh, "density": dens,
            "z_distinct": z1, "z_out_hh": z2, "z_density": z3
        }

    # --- è¿‘ä¼¼æº–ç¢ºæ€§è¨ºæ–·ï¼ˆç†è«– + å³æ™‚ï¼‰---
    def approximation_diagnostics(self) -> Dict[str, Dict[str, float]]:
        """
        å›å‚³ KMV / CMS çš„ã€Œç†è«–ç´šã€å³æ™‚è¨ºæ–·ï¼š
        KMVï¼š
          - k, n_samples, r_k, rel_error_theory â‰ˆ 1/sqrt(k)
        CMSï¼š
          - w, d, epsilon=1/w, delta=exp(-d)
          - N_out/N_inï¼ˆç¸½æ›´æ–°å€¼ï¼‰
          - bound_out = epsilon * N_outï¼ˆå°ä»»ä½• out æŸ¥è©¢çš„æœ€å¤§é«˜ä¼°åå·®ï¼‰
          - bound_in  = epsilon * N_in
        """
        kmv_diag = self.kmv.diagnostics()
        eps = 1.0 / float(self.cms_out.w)
        delta = math.exp(-float(self.cms_out.d))
        return {
            "kmv": {
                "k": kmv_diag["k"] or 0.0,
                "n_samples": kmv_diag["n_samples"] or 0.0,
                "r_k": kmv_diag["r_k"] if kmv_diag["r_k"] is not None else -1.0,
                "rel_error_theory": kmv_diag["rel_error_theory"] or 0.0,
            },
            "cms": {
                "w": float(self.cms_out.w),
                "d": float(self.cms_out.d),
                "epsilon": eps,
                "delta": delta,
                "N_out": float(self.total_out_flow),
                "N_in":  float(self.total_in_flow),
                "bound_out": eps * float(self.total_out_flow),
                "bound_in":  eps * float(self.total_in_flow),
            }
        }


# =========================
# UF-FAE å¼•æ“ï¼šé¬†å¼›â†’æ”¶ç·Š
# =========================

class UF_FAE_RuleEngine:
    """
    ä¸»æµç¨‹ï¼š
    - step_edge(): æŠŠ pending å¸æ”¶ gap äº‚åºä¸¦å¯¦éš›æ›´æ–° DSU / ç¾¤çµ„çµ±è¨ˆ
    - WCC åˆä½µæ™‚è¨ˆç®— Sï¼Œè‹¥é«˜æ–¼é–€æª» â†’ å°è©² WCC åšäº’æƒ ç²¾ç…‰ï¼ˆåƒ…æœ€è¿‘çª—å£ï¼‰
    - æä¾› metrics_summary() ä¾›é¢æ¿åˆ—å°ï¼ˆå« KMV/CMS æº–ç¢ºæ€§è¨ºæ–·ï¼‰
    """
    def __init__(self, kmv_k=64, cms_w=2048, cms_d=4, window_edges=2_000_000, gap_size=0):
        self.dsu = DSU()
        self.comps: Dict[Any, ComponentStats] = {}
        self.pending: Deque[Tuple[float, Any, Any, float]] = deque()
        self.window: Deque[Tuple[float, Any, Any, float]] = deque(maxlen=window_edges)
        self.gap = gap_size
        self.kmv_k, self.cms_w, self.cms_d = kmv_k, cms_w, cms_d

        # é¢æ¿éœ€è¦çš„ä¸€äº›ç·©è¡
        self.recent_senders: Deque[Any] = deque(maxlen=RECENT_SENDER_BUFFER)
        self.last_merge_metrics: Optional[Dict[str, float]] = None  # æœ€è¿‘åˆä½µçš„ S èˆ‡æ˜ç´°

    # ---- å…§éƒ¨å·¥å…· ----
    def _comp(self, root):
        if root not in self.comps:
            self.comps[root] = ComponentStats(self.kmv_k, self.cms_w, self.cms_d)
        return self.comps[root]

    def _merge_stats(self, new_root, old_root):
        """Union ä¹‹å¾ŒæŠŠ old_root çš„çµ±è¨ˆä½µåˆ° new_rootã€‚"""
        a = self._comp(new_root); b = self.comps.get(old_root)
        if not b: return
        # KMVï¼šæŠŠæ¨£æœ¬é€ä¸€åŠ å…¥ï¼ˆè¿‘ä¼¼ï¼‰
        for hv in b.kmv.samples: a.kmv.add(hv)
        # CMSï¼šè¡¨æ ¼é€æ ¼ç›¸åŠ ï¼ˆè¡Œåˆ—ä¸€è‡´ï¼‰
        for i in range(a.cms_out.d):
            ai, bi = a.cms_out.tables[i], b.cms_out.tables[i]
            for j in range(a.cms_out.w): ai[j] += bi[j]
            ai2, bi2 = a.cms_in.tables[i], b.cms_in.tables[i]
            for j in range(a.cms_in.w): ai2[j] += bi2[j]
        a.nodes |= b.nodes
        a.edge_cnt += b.edge_cnt
        a.total_out_flow += b.total_out_flow
        a.total_in_flow  += b.total_in_flow
        for h in b.history: a.history.append(h)
        del self.comps[old_root]

    def _update_edge(self, u, v, amount):
        """å¯¦éš›æŠŠ u-v åˆä½µã€æ›´æ–°çµ±è¨ˆï¼›å›å‚³ (merged?, root)ã€‚"""
        merged, new_root, old_root = self.dsu.union(u, v)
        root = self.dsu.find(u)
        comp = self._comp(root)
        comp.update_on_edge(u, v, amount)
        if merged:
            self._merge_stats(new_root, old_root)
        return merged, root

    # ---- ä¸»æ¨é€² ----
    def step_edge(self, t, u, v, amount, thr_mid=2.0, thr_high=3.0):
        """
        é€²ä¾†ä¸€æ¢é‚Š â†’ å…ˆä¸Ÿ pendingï¼›pending è¶…é gap â†’ å–å‡ºæœ€èˆŠé‚£æ¢å¯¦éš›æ›´æ–°ã€‚
        è‹¥ç™¼ç”Ÿå…ƒä»¶åˆä½µï¼ˆmerged=Trueï¼‰ï¼Œå‰‡è¨ˆç®— S èˆ‡è­¦ç¤ºï¼›å¿…è¦æ™‚åšäº’æƒ ç²¾ç…‰ã€‚
        """
        alerts = []
        self.pending.append((t,u,v,amount))
        if len(self.pending) > self.gap:
            t0,u0,v0,a0 = self.pending.popleft()
            merged, root = self._update_edge(u0, v0, a0)
            self.window.append((t0,u0,v0,a0))
            self.recent_senders.append(u0)

            if merged:
                comp = self._comp(root)
                S, d = comp.snapshot_and_score(src_key=u0)

                # é¢¨éšªç­‰ç´š
                level = "LOW"
                if S >= thr_high: level = "HIGH"
                elif S >= thr_mid: level = "MED"

                # è­¦ç¤ºç´€éŒ„
                alerts.append({
                    "time": t0, "root": str(root), "src": str(u0), "dst": str(v0),
                    "amount": float(a0), "S": float(round(S,4)), "level": level,
                    "distinct": float(round(d["distinct"],3)),
                    "out_hh":   float(round(d["out_hh"],3)),
                    "density":  float(round(d["density"],6)),
                    "z_distinct": float(round(d["z_distinct"],3)),
                    "z_out_hh":  float(round(d["z_out_hh"],3)),
                    "z_density": float(round(d["z_density"],3)),
                })

                # æœ€è¿‘ä¸€æ¬¡åˆä½µæ˜ç´°ï¼ˆé¢æ¿ç”¨ï¼‰
                self.last_merge_metrics = {
                    "root": str(root),
                    "src": str(u0),
                    "dst": str(v0),
                    "S": float(round(S,4)),
                    "z_distinct": float(round(d["z_distinct"],3)),
                    "z_out_hh": float(round(d["z_out_hh"],3)),
                    "z_density": float(round(d["z_density"],3)),
                    "distinct": float(round(d["distinct"],3)),
                    "out_hh": float(round(d["out_hh"],3)),
                    "density": float(round(d["density"],6)),
                }

                # è§¸ç™¼äº’æƒ ç²¾ç…‰ï¼ˆå±€éƒ¨ï¼‰
                try:
                    if REFINE_ON_S and S >= REFINE_S_THRESH:
                        self.refine_component_by_reciprocal(root, window_limit=RECIP_WINDOW)
                except Exception as e:
                    print(f"[REFINE][WARN] {e}")
        return alerts

    def flush_all(self, thr_mid=2.0, thr_high=3.0):
        """æŠŠ pending å…¨éƒ¨å¯¦éš›æ›´æ–°ï¼ˆçµå°¾æ¸…å€‰ï¼‰ï¼›æ³¨æ„åˆ¥å† enqueueã€‚"""
        alerts = []
        while self.pending:
            t0, u0, v0, a0 = self.pending.pop()  # å¾å°¾ç«¯å–ï¼šå°äº‚åºå½±éŸ¿å°
            merged, root = self._update_edge(u0, v0, a0)
            self.window.append((t0, u0, v0, a0))
            self.recent_senders.append(u0)

            if merged:
                comp = self._comp(root)
                S, d = comp.snapshot_and_score(src_key=u0)

                level = "LOW"
                if S >= thr_high: level = "HIGH"
                elif S >= thr_mid: level = "MED"

                alerts.append({
                    "time": t0, "root": str(root), "src": str(u0), "dst": str(v0),
                    "amount": float(a0), "S": float(round(S,4)), "level": level,
                    "distinct": float(round(d["distinct"],3)),
                    "out_hh":   float(round(d["out_hh"],3)),
                    "density":  float(round(d["density"],6)),
                    "z_distinct": float(round(d["z_distinct"],3)),
                    "z_out_hh":  float(round(d["z_out_hh"],3)),
                    "z_density": float(round(d["z_density"],3)),
                })

                self.last_merge_metrics = {
                    "root": str(root),
                    "src": str(u0),
                    "dst": str(v0),
                    "S": float(round(S,4)),
                    "z_distinct": float(round(d["z_distinct"],3)),
                    "z_out_hh": float(round(d["z_out_hh"],3)),
                    "z_density": float(round(d["z_density"],3)),
                    "distinct": float(round(d["distinct"],3)),
                    "out_hh": float(round(d["out_hh"],3)),
                    "density": float(round(d["density"],6)),
                }

                try:
                    if REFINE_ON_S and S >= REFINE_S_THRESH:
                        self.refine_component_by_reciprocal(root, window_limit=RECIP_WINDOW)
                except Exception as e:
                    print(f"[REFINE][WARN] {e}")
        return alerts

    # ---- äº’æƒ ç²¾ç…‰ï¼šWCCï¼ˆé¬†ï¼‰â†’ æœ‰å‘å­å¡Šï¼ˆç·Šï¼‰----
    def refine_component_by_reciprocal(self, root, window_limit=200_000):
        """
        å°æŒ‡å®š root çš„ WCC åšè¼•é‡æ”¶ç·Šï¼š
        1) æ”¶é›†æœ€è¿‘ window_limit ç­†å…§å±¬æ–¼æ­¤å…ƒä»¶çš„é‚Šã€‚
        2) ä»¥ã€Œäº’æƒ ã€(u->v & v->u) çš„å…©å‘é‚Šç•¶åˆä½µæ¢ä»¶ï¼Œå±€éƒ¨é‡å»º DSUã€‚
        3) ä¾æ–° DSU åˆ†çµ„ï¼Œé‡å»º ComponentStatsï¼Œå–ä»£èˆŠ rootã€‚
        """
        edges: List[Tuple[Any,Any,float]] = []
        nodes_in_root: Set[Any] = set()
        seen = 0
        for (t,u,v,a) in reversed(self.window):
            if seen >= window_limit:
                break
            if self.dsu.find(u) == root or self.dsu.find(v) == root:
                edges.append((u,v,a))
                nodes_in_root.add(u); nodes_in_root.add(v)
                seen += 1
        if not edges or len(nodes_in_root) < MIN_SUBCOMP_NODES:
            return

        forward = set((u,v) for (u,v,_) in edges)
        reciprocal_pairs = set((u,v) for (u,v) in forward if (v,u) in forward)
        if not reciprocal_pairs:
            return

        # å±€éƒ¨é‡å»º
        self.dsu.reset_subset(nodes_in_root)
        for (u,v) in reciprocal_pairs:
            self.dsu.union(u, v)

        # æ–°åˆ†çµ„ï¼ˆåƒ…ä¿ç•™åŒå­å¡Šå…§çš„é‚Šï¼‰
        groups: Dict[Any, List[Tuple[Any,Any,float]]] = defaultdict(list)
        for (u,v,a) in edges:
            ru = self.dsu.find(u)
            rv = self.dsu.find(v)
            if ru == rv:
                groups[ru].append((u,v,a))

        old_comp = self.comps.get(root)
        if root in self.comps:
            del self.comps[root]

        made = 0
        for sub_root, es in groups.items():
            sub_nodes = set()
            for (u,v,a) in es:
                sub_nodes.add(u); sub_nodes.add(v)
            if len(sub_nodes) < MIN_SUBCOMP_NODES:
                continue
            comp_new = ComponentStats(self.kmv_k, self.cms_w, self.cms_d)
            for (u,v,a) in es:
                comp_new.update_on_edge(u, v, a)
            self.comps[sub_root] = comp_new
            made += 1

        if made == 0 and old_comp is not None:
            # è‹¥æ²’æœ‰åˆæ³•å­å¡Šï¼Œé¿å…æ•´å€‹å…ƒä»¶æ¶ˆå¤±
            self.comps[root] = old_comp

    # ---- æŒ‡æ¨™é¢æ¿ï¼šç°¡æ½” + ç´…é»ƒç¶ ç‡ˆ + è¿‘ä¼¼æº–ç¢ºæ€§ ----
    def metrics_summary(self, topk: int = TOPK_HEAVY_HITTERS) -> Dict[str, Any]:
        """
        å›å‚³é¢æ¿æ‰€éœ€æ‘˜è¦ï¼š
        - å…ƒä»¶æ•¸ã€æœ€å¤§ç¾¤çµ„ï¼ˆdistinct~KMVï¼‰ã€æœ€ç·Šç¾¤çµ„ï¼ˆå¯†åº¦ï¼‰
        - è¿‘ç«¯ sender æŠ½æ¨£çš„ HH(out) Top-Kï¼ˆå«å¯èƒ½åå·®ä¸Šç•Œï¼‰
        - æœ€è¿‘ä¸€æ¬¡åˆä½µçš„ S è·Ÿ z åˆ†æ•¸
        - è¿‘ä¼¼æº–ç¢ºæ€§æŒ‡æ¨™ï¼ˆKMV / CMSï¼‰
        """
        # (1) æœ€å¤§ & æœ€ç·Š
        num_components = len(self.comps)
        largest_root = None
        largest_distinct = -1.0
        densest_root = None
        largest_density = 0.0

        for r, comp in self.comps.items():
            d = comp.approx_distinct()
            if d > largest_distinct:
                largest_distinct = d
                largest_root = r
            rho = comp.approx_density()
            if rho > largest_density:
                largest_density = rho
                densest_root = r

        # (2) æŠ½æ¨£ HHï¼ˆç”¨æœ€è¿‘ sender ç·©è¡ï¼‰
        uniq = list(dict.fromkeys(self.recent_senders))  # ä¿åºå»é‡
        est_list = []
        for u in uniq:
            root = self.dsu.find(u)
            comp = self.comps.get(root)
            if not comp:
                continue
            est_out = comp.approx_out_hh(u)
            # å°é€™å€‹ã€Œç¾¤çµ„ã€çš„ CMS èª¤å·®ä¸Šç•Œï¼ˆé«˜ä¼°ï¼‰ï¼ Îµ * N_out
            eps = 1.0 / float(comp.cms_out.w)
            bound = eps * float(comp.total_out_flow)
            est_list.append((est_out, str(u), str(root), bound))
        est_list.sort(key=lambda x: x[0], reverse=True)
        hh_top = est_list[:topk]

        # (3) æœ€è¿‘ä¸€æ¬¡åˆä½µ
        lm = self.last_merge_metrics

        # (4) è¿‘ä¼¼æº–ç¢ºæ€§è¨ºæ–·ï¼ˆä»¥ã€Œæœ€å¤§ç¾¤çµ„ã€ç‚ºä»£è¡¨ï¼›ä¹Ÿå¯ä»¥æ“´å±•æˆå¤šç¾¤çµ„ï¼‰
        approx_diag = None
        if largest_root is not None and largest_root in self.comps:
            approx_diag = self.comps[largest_root].approximation_diagnostics()
            approx_diag["represent_root"] = str(largest_root)

        return {
            "num_components": num_components,
            "largest_root": str(largest_root) if largest_root is not None else None,
            "largest_distinct": float(largest_distinct) if largest_distinct >= 0 else None,
            "densest_root": str(densest_root) if densest_root is not None else None,
            "largest_density": float(largest_density),
            "heavy_hitters_top": hh_top,  # list of (est_out, sender, root, bound_over)
            "last_merge": lm,             # dict or None
            "approx_diagnostics": approx_diag,  # KMV / CMS è¨ºæ–·
        }


# =========================
# Polars å°å·¥å…·ï¼ˆæ™‚é–“æ¬„ä½è§£æï¼‰
# =========================

def find_col(names, candidates):
    """åœ¨ names è£¡æ‰¾ç¬¬ä¸€å€‹ç¬¦åˆ candidates çš„æ¬„ä½ï¼ˆå¤§å°å¯«ç„¡é—œï¼‰"""
    lower_map = {n.lower(): n for n in names}
    for c in candidates:
        key = c.lower()
        if key in lower_map:
            return lower_map[key]
    return None


def normalize_time_expr(time_col_name: str):
    """
    ç”¢ç”Ÿ 'tnum'ï¼ˆfloat ç§’ï¼‰ï¼Œé¿å…æ˜‚è²´æ¨æ–·ï¼š
      A) ç›´æ¥æ•¸å€¼ï¼ˆfloat / intï¼‰
      B) ç´”æ•¸å­—å­—ä¸² â†’ epochï¼ˆè‡ªå‹•åˆ¤æ–·ç§’/æ¯«ç§’ï¼‰
      C) åƒ…ä½¿ç”¨æ˜ç¢º format çš„ strptimeï¼ˆstrict=Falseï¼‰
    å…¨å¤±æ•— â†’ nullï¼ˆä¸»è¿´åœˆç”¨æµæ°´è™Ÿè£œï¼‰
    """
    t = pl.col(time_col_name)
    t_utf8 = t.cast(pl.Utf8, strict=False)

    numeric = pl.coalesce([
        t.cast(pl.Float64, strict=False),
        t.cast(pl.Int64,   strict=False).cast(pl.Float64, strict=False),
    ])

    digits = (
        t_utf8
        .str.replace_all(r"[^0-9]", "")
        .cast(pl.Float64, strict=False)
    )
    epoch_digits = (
        pl.when((digits.is_not_null()) & (digits > 0))
        .then(pl.when(digits > 1_000_000_000_000.0).then(digits / 1000.0).otherwise(digits))
        .otherwise(None)
        .cast(pl.Float64, strict=False)
    )

    fmts = [
        "%Y-%m-%d %H:%M:%S",
        "%Y/%m/%d %H:%M:%S",
        "%Y-%m-%d %H:%M",
        "%Y/%m/%d %H:%M",
        "%Y-%m-%d",
        "%Y/%m/%d",
        "%m/%d/%Y %H:%M:%S",
        "%m/%d/%Y %H:%M",
        "%m/%d/%Y",
    ]
    parsed_epochs = [
        t_utf8
        .str.strptime(pl.Datetime, format=fmt, strict=False, exact=False)
        .dt.epoch(time_unit="s")
        .cast(pl.Float64, strict=False)
        for fmt in fmts
    ]

    return pl.coalesce([numeric, epoch_digits, *parsed_epochs]).alias("tnum")


def row_iter():
    """
    å›å‚³é€åˆ—è¿­ä»£å™¨ï¼š(tnum, sender, receiver, amount)
    - è‡ªå‹•åµæ¸¬å¯¦éš›æ¬„ä½åï¼ˆå¤§å°å¯«ç„¡é—œ + å¸¸è¦‹åˆ¥åï¼‰
    - ASSUME_SORTED=Trueï¼šengine="streaming"ï¼ˆä½è¨˜æ†¶é«”ï¼‰
      å¦å‰‡ï¼šsort å† collectï¼ˆengine="auto"ï¼‰
    """
    lf0 = pl.scan_parquet(INPUT_PARQUET)
    schema = lf0.collect_schema()
    names = list(schema.keys())

    time_actual = find_col(names, [
        TIME_COL, "time", "Time", "timestamp", "Timestamp",
        "datetime", "Datetime", "date", "Date"
    ])
    sender_actual = find_col(names, [
        SENDER_COL, "sender", "Sender", "sender_account", "Sender_account",
        "src", "from", "From", "payer", "Payer"
    ])
    receiver_actual = find_col(names, [
        RECEIVER_COL, "receiver", "Receiver", "receiver_account", "Receiver_account",
        "dst", "to", "To", "payee", "Payee"
    ])
    amount_actual = find_col(names, [
        AMOUNT_COL, "amount", "Amount", "value", "Value",
        "payment_amount", "Payment_amount", "amt", "Amt"
    ])

    missing = [k for k, v in [
        ("time", time_actual),
        ("sender", sender_actual),
        ("receiver", receiver_actual),
        ("amount", amount_actual),
    ] if v is None]
    if missing:
        print("[SCHEMA] æª”æ¡ˆæ¬„ä½ï¼š", names)
        raise RuntimeError(f"[SCHEMA] æ‰¾ä¸åˆ°å¿…è¦æ¬„ä½ï¼š{missing}ï¼›è«‹èª¿æ•´åˆ¥åæˆ–æª”æ¡ˆæ¬„ä½ã€‚")

    lf = lf0.select([
        normalize_time_expr(time_actual),
        pl.col(sender_actual).alias("sender"),
        pl.col(receiver_actual).alias("receiver"),
        pl.col(amount_actual).cast(pl.Float64, strict=False).alias("amount"),
    ])

    if ASSUME_SORTED:
        df = lf.collect(engine="streaming")
    else:
        df = lf.sort("tnum").collect(engine="auto")

    return df.iter_rows(named=True)


# =========================
# ä¸»ç¨‹å¼ï¼šä¸²æµè™•ç† + é¢æ¿ + è¿‘ä¼¼è¨ºæ–·
# =========================

def _level_emoji(level: str) -> str:
    """æŠŠç­‰ç´šè½‰ç´…é»ƒç¶ ç‡ˆ emojiã€‚"""
    return {"HIGH":"ğŸŸ¥", "MED":"ğŸŸ§", "LOW":"ğŸŸ©"}.get(level, "â¬œ")

def _short_dx(zD, zHH, zRho) -> str:
    """ä¸€å¥è©±è¨ºæ–·ï¼šå“ªäº›ç¶­åº¦åœ¨æ‹‰è­¦å ±ã€‚"""
    parts = []
    if zD >= 2.0: parts.append("è¦æ¨¡æ“´å¼µ")
    if zHH >= 2.0: parts.append("å‡ºé‡‘é›†ä¸­")
    if zRho >= 2.0: parts.append("ç¾¤å…§ç·Šå¯†")
    if not parts: return "å¹³ç©©"
    if len(parts) == 1: return f"{parts[0]}"
    if len(parts) == 2: return f"{parts[0]} + {parts[1]}"
    return "è¦æ¨¡+é›†ä¸­+ç·Šå¯†ï¼ˆé«˜åº¦å¯ç–‘ï¼‰"

def print_metrics(engine: UF_FAE_RuleEngine, alerted: int, cnt: int):
    """äººè®€å‹å–„çš„é¢æ¿ï¼ŒåŒ…å«è¿‘ä¼¼æº–ç¢ºæ€§æŒ‡æ¨™ã€‚"""
    metrics = engine.metrics_summary()
    hh_str = ", ".join([
        f"{i+1}:{sender}/root={root}â‰ˆ{int(val)} (â‰¤+{int(bound)})"
        for i,(val,sender,root,bound) in enumerate(metrics["heavy_hitters_top"])
    ])

    print(f"[PROGRESS] edges={cnt:,} alerts={alerted:,}")
    print(
        "[METRICS] comps={:,} | largest(V)={}@{} | densest={} Ï={:.6f} | HH(out) [{}]".format(
            metrics["num_components"],
            int(metrics["largest_distinct"] or 0), metrics["largest_root"],
            metrics["densest_root"], metrics["largest_density"],
            hh_str,
        )
    )

    # æœ€è¿‘ä¸€æ¬¡åˆä½µçš„åˆ†æ•¸èˆ‡è¨ºæ–·
    if metrics["last_merge"]:
        lm = metrics["last_merge"]
        level = "HIGH" if lm["S"] >= THRESH_HIGH else ("MED" if lm["S"] >= THRESH_MID else "LOW")
        print(
            "          [LAST-MERGE] {} root={} src={} dst={} | S={:.3f} | "
            "zD={:.2f} zHH={:.2f} zÏ={:.2f} | D~{} HH~{} Ï~{:.6f} | è¨ºæ–·: {}".format(
                _level_emoji(level),
                lm["root"], lm["src"], lm["dst"], lm["S"],
                lm["z_distinct"], lm["z_out_hh"], lm["z_density"],
                lm["distinct"], lm["out_hh"], lm["density"],
                _short_dx(lm["z_distinct"], lm["z_out_hh"], lm["z_density"])
            )
        )

    # è¿‘ä¼¼è¨ºæ–·ï¼ˆä»¥æœ€å¤§ç¾¤çµ„ç‚ºä»£è¡¨ï¼‰
    if metrics["approx_diagnostics"]:
        ad = metrics["approx_diagnostics"]
        kmv = ad["kmv"]; cms = ad["cms"]
        print(
            "          [APPROX] root={} | KMV: k={} samples={} r_k={} æœŸæœ›ç›¸å°èª¤å·®â‰ˆ{:.3f} | "
            "CMS: w={} d={} Îµ=1/wâ‰ˆ{:.6f} Î´â‰ˆe^-dâ‰ˆ{:.6f} | N_outâ‰ˆ{:.0f} â†’ ä»»ä½•ä¼°å€¼é«˜ä¼°ä¸Šç•Œ â‰¤ Îµ*N_outâ‰ˆ{:.0f}".format(
                ad.get("represent_root","?"),
                int(kmv["k"]), int(kmv["n_samples"]), int(kmv["r_k"]) if kmv["r_k"]>=0 else "NA",
                float(kmv["rel_error_theory"]),
                int(cms["w"]), int(cms["d"]), float(cms["epsilon"]), float(cms["delta"]),
                float(cms["N_out"]), float(cms["bound_out"])
            )
        )

def main():
    print("[INFO] reading:", INPUT_PARQUET)

    # å¯é¸ï¼šè‹¥è¦å¼·åˆ¶ä½¿ç”¨ 16 åŸ·è¡Œç·’ï¼ˆPolars/Rayonï¼‰ï¼Œå¯åœ¨å¤–éƒ¨è¨­å®šç’°å¢ƒè®Šæ•¸å¾Œå†åŸ·è¡Œã€‚
    os.environ.setdefault("POLARS_MAX_THREADS", "16")
    os.environ.setdefault("RAYON_NUM_THREADS", "16")

    # å˜—è©¦å–å¾—ç¸½åˆ—æ•¸ä»¥ç¾åŒ–é€²åº¦æ¢ï¼›å¤±æ•—å°±ç”¨ä¸å®šé•·åº¦
    try:
        total_edges = pl.scan_parquet(INPUT_PARQUET).select(pl.count()).collect().item()
    except Exception:
        total_edges = None

    engine = UF_FAE_RuleEngine(
        kmv_k=KMV_K, cms_w=CMS_W, cms_d=CMS_D,
        window_edges=WINDOW_EDGES, gap_size=GAP_SIZE
    )

    header = ["time","root","src","dst","amount","S","level",
              "distinct","out_hh","density","z_distinct","z_out_hh","z_density"]
    if os.path.exists(OUTPUT_ALERTS):
        os.remove(OUTPUT_ALERTS)
    f = open(OUTPUT_ALERTS, "w", newline="", encoding="utf-8")
    w = csv.DictWriter(f, fieldnames=header); w.writeheader()

    cnt = 0
    alerted = 0
    pbar = tqdm(total=total_edges, unit="edge", dynamic_ncols=True, desc="UF-FAE streaming")
    alerts_buffer: List[Dict[str, Any]] = []

    for r in row_iter():
        # å–æ¬„ä½ï¼ˆtnum å¤±æ•—æ™‚ä»¥æµæ°´è™Ÿè£œï¼‰
        t = float(r["tnum"]) if r["tnum"] is not None else float(cnt)
        u = r["sender"]; v = r["receiver"]
        amt = float(r["amount"]) if r["amount"] is not None else 0.0

        # ä¸»æ¨é€²
        alerts = engine.step_edge(t,u,v,amt, thr_mid=THRESH_MID, thr_high=THRESH_HIGH)
        if alerts:
            alerts_buffer.extend(alerts)
            alerted += len(alerts)
            # æ‰¹æ¬¡å¯«æª”ï¼Œæ¸›å°‘ I/O æ¬¡æ•¸
            if len(alerts_buffer) >= CSV_FLUSH_EVERY:
                w.writerows(alerts_buffer)
                alerts_buffer.clear()

        # é€²åº¦èˆ‡é¢æ¿ç¯€æµ
        cnt += 1
        if cnt % TQDM_UPDATE_EVERY == 0:
            pbar.update(TQDM_UPDATE_EVERY)

        if cnt % PRINT_METRICS_EVERY == 0:
            pbar.set_postfix_str(f"alerts={alerted:,} comps={engine.metrics_summary()['num_components']}")
            print_metrics(engine, alerted, cnt)

    # æ”¶å°¾ï¼šè™•ç†æ®˜é¤˜ pending
    tail_alerts = engine.flush_all(thr_mid=THRESH_MID, thr_high=THRESH_HIGH)
    if tail_alerts:
        alerts_buffer.extend(tail_alerts)
        alerted += len(tail_alerts)

    # æŠŠç·©è¡ä¸­çš„è­¦ç¤ºä¸€æ¬¡å¯«å‡º
    if alerts_buffer:
        w.writerows(alerts_buffer)
        alerts_buffer.clear()

    f.flush(); f.close()
    print_metrics(engine, alerted, cnt)
    print(f"[DONE] total edges={cnt:,}, alerts={alerted:,}, output -> {OUTPUT_ALERTS}")


if __name__ == "__main__":
    main()
